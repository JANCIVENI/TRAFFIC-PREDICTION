{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "7w-Yt9qBgIi7",
        "outputId": "24f5a7d3-0204-41bc-b18e-39a06a80ef79"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 37)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    mape = MAPE(y_true, y_pred)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from data.data import process_data\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import sklearn.metrics as metrics\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def MAPE(y_true, y_pred):\n",
        "    \"\"\"Mean Absolute Percentage Error\n",
        "    Calculate the mape.\n",
        "    # Arguments\n",
        "        y_true: List/ndarray, ture data.\n",
        "        y_pred: List/ndarray, predicted data.\n",
        "    # Returns\n",
        "        mape: Double, result data for train.\n",
        "    \"\"\"\n",
        "    y = [x for x in y_true if x > 0]\n",
        "    y_pred = [y_pred[i] for i in range(len(y_true)) if y_true[i] > 0]\n",
        "    num = len(y_pred)\n",
        "    sums = 0\n",
        "    for i in range(num):\n",
        "        tmp = abs(y[i] - y_pred[i]) / y[i]\n",
        "        sums += tmp\n",
        "    mape = sums * (100 / num)\n",
        "    return mape\n",
        "def eva_regress(y_true, y_pred):\n",
        "    \"\"\"Evaluation\n",
        "    evaluate the predicted resul.\n",
        "    # Arguments\n",
        "        y_true: List/ndarray, ture data.\n",
        "        y_pred: List/ndarray, predicted data.\n",
        "   \"\"\"\n",
        "   mape = MAPE(y_true, y_pred)\n",
        "    vs = metrics.explained_variance_score(y_true, y_pred)\n",
        "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
        "    r2 = metrics.r2_score(y_true, y_pred)\n",
        "    print('explained_variance_score:%f' % vs)\n",
        "    print('mape:%f%%' % mape)\n",
        "    print('mae:%f' % mae)\n",
        "    print('mse:%f' % mse)\n",
        "    print('rmse:%f' % math.sqrt(mse))\n",
        "    print('r2:%f' % r2)\n",
        "def plot_results(y_true, y_preds, names):\n",
        "    \"\"\"Plot\n",
        "    Plot the true data and predicted data.\n",
        "    # Arguments\n",
        "        y_true: List/ndarray, ture data.\n",
        "        y_pred: List/ndarray, predicted data.\n",
        "        names: List, Method names.\n",
        "    \"\"\"\n",
        "    d = '2016-3-4 00:00'\n",
        "    x = pd.date_range(d, periods=288, freq='5min')\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    ax.plot(x, y_true, label='True Data')\n",
        "    for name, y_pred in zip(names, y_preds):\n",
        "        ax.plot(x, y_pred, label=name)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Time of Day')\n",
        "    plt.ylabel('Flow')\n",
        "\n",
        "    date_format = mpl.dates.DateFormatter(\"%H:%M\")\n",
        "    ax.xaxis.set_major_formatter(date_format)\n",
        "    fig.autofmt_xdate()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    lstm = load_model('model/lstm.h5')\n",
        "    gru = load_model('model/gru.h5')\n",
        "    saes = load_model('model/saes.h5')\n",
        "    models = [lstm, gru, saes]\n",
        "    names = ['LSTM', 'GRU', 'SAEs']\n",
        "\n",
        "    lag = 12\n",
        "    file1 = 'data/train.csv'\n",
        "    file2 = 'data/test.csv'\n",
        "    _, _, X_test, y_test, scaler = process_data(file1, file2, lag)\n",
        "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "\n",
        "    y_preds = []\n",
        "    for name, model in zip(names, models):\n",
        "        if name == 'SAEs':\n",
        "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]))\n",
        "        else:\n",
        "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "        file = 'images/' + name + '.png'\n",
        "        plot_model(model, to_file=file, show_shapes=True)\n",
        "        predicted = model.predict(X_test)\n",
        "        predicted = scaler.inverse_transform(predicted.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "        y_preds.append(predicted[:288])\n",
        "        print(name)\n",
        "        eva_regress(y_test, predicted)\n",
        "\n",
        "    plot_results(y_test[: 288], y_preds, names)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train the NN model.\n",
        "\"\"\"\n",
        "import sys\n",
        "import warnings\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from data.data import process_data\n",
        "from model import model\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def train_model(model, X_train, y_train, name, config):\n",
        "    \"\"\"train\n",
        "    train a single model.\n",
        "\n",
        "    # Arguments\n",
        "        model: Model, NN model to train.\n",
        "        X_train: ndarray(number, lags), Input data for train.\n",
        "        y_train: ndarray(number, ), result data for train.\n",
        "        name: String, name of model.\n",
        "        config: Dict, parameter for train.\n",
        "    \"\"\"\n",
        "\n",
        "    model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['mape'])\n",
        "    # early = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='auto')\n",
        "    hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=config[\"batch\"],\n",
        "        epochs=config[\"epochs\"],\n",
        "        validation_split=0.05)\n",
        "\n",
        "    model.save('model/' + name + '.h5')\n",
        "    df = pd.DataFrame.from_dict(hist.history)\n",
        "    df.to_csv('model/' + name + ' loss.csv', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "def train_seas(models, X_train, y_train, name, config):\n",
        "    \"\"\"train\n",
        "    train the SAEs model.\n",
        "\n",
        "    # Arguments\n",
        "        models: List, list of SAE model.\n",
        "        X_train: ndarray(number, lags), Input data for train.\n",
        "        y_train: ndarray(number, ), result data for train.\n",
        "        name: String, name of model.\n",
        "        config: Dict, parameter for train.\n",
        "    \"\"\"\n",
        "\n",
        "    temp = X_train\n",
        "    # early = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='auto')\n",
        "\n",
        "    for i in range(len(models) - 1):\n",
        "        if i > 0:\n",
        "            p = models[i - 1]\n",
        "            hidden_layer_model = Model(input=p.input,\n",
        "                                       output=p.get_layer('hidden').output)\n",
        "            temp = hidden_layer_model.predict(temp)\n",
        "\n",
        "        m = models[i]\n",
        "        m.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['mape'])\n",
        "\n",
        "        m.fit(temp, y_train, batch_size=config[\"batch\"],\n",
        "              epochs=config[\"epochs\"],\n",
        "              validation_split=0.05)\n",
        "\n",
        "        models[i] = m\n",
        "\n",
        "    saes = models[-1]\n",
        "    for i in range(len(models) - 1):\n",
        "        weights = models[i].get_layer('hidden').get_weights()\n",
        "        saes.get_layer('hidden%d' % (i + 1)).set_weights(weights)\n",
        "\n",
        "    train_model(saes, X_train, y_train, name, config)\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        default=\"lstm\",\n",
        "        help=\"Model to train.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    lag = 12\n",
        "    config = {\"batch\": 256, \"epochs\": 600}\n",
        "    file1 = 'data/train.csv'\n",
        "    file2 = 'data/test.csv'\n",
        "    X_train, y_train, _, _, _ = process_data(file1, file2, lag)\n",
        "\n",
        "    if args.model == 'lstm':\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "        m = model.get_lstm([12, 64, 64, 1])\n",
        "        train_model(m, X_train, y_train, args.model, config)\n",
        "    if args.model == 'gru':\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "        m = model.get_gru([12, 64, 64, 1])\n",
        "        train_model(m, X_train, y_train, args.model, config)\n",
        "    if args.model == 'saes':\n",
        "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]))\n",
        "        m = model.get_saes([12, 400, 400, 400, 1])\n",
        "        train_seas(m, X_train, y_train, args.model, config)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(sys.argv)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "zh6kdljlQTxc",
        "outputId": "85219a7c-4b74-4aac-d99b-7e6110e3ce51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data.data'; 'data' is not a package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8884267353de>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data.data'; 'data' is not a package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Defination of NN model\n",
        "\"\"\"\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "def get_lstm(units):\n",
        "    \"\"\"LSTM(Long Short-Term Memory)\n",
        "    Build LSTM Model.\n",
        "\n",
        "    # Arguments\n",
        "        units: List(int), number of input, output and hidden units.\n",
        "    # Returns\n",
        "        model: Model, nn model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
        "    model.add(LSTM(units[2]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units[3], activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_gru(units):\n",
        "    \"\"\"GRU(Gated Recurrent Unit)\n",
        "    Build GRU Model.\n",
        "\n",
        "    # Arguments\n",
        "        units: List(int), number of input, output and hidden units.\n",
        "    # Returns\n",
        "        model: Model, nn model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
        "    model.add(GRU(units[2]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units[3], activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_sae(inputs, hidden, output):\n",
        "    \"\"\"SAE(Auto-Encoders)\n",
        "    Build SAE Model.\n",
        "\n",
        "    # Arguments\n",
        "        inputs: Integer, number of input units.\n",
        "        hidden: Integer, number of hidden units.\n",
        "        output: Integer, number of output units.\n",
        "    # Returns\n",
        "        model: Model, nn model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden, input_dim=inputs, name='hidden'))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_saes(layers):\n",
        "    \"\"\"SAEs(Stacked Auto-Encoders)\n",
        "    Build SAEs Model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: List(int), number of input, output and hidden units.\n",
        "    # Returns\n",
        "        models: List(Model), List of SAE and SAEs.\n",
        "    \"\"\"\n",
        "    sae1 = _get_sae(layers[0], layers[1], layers[-1])\n",
        "    sae2 = _get_sae(layers[1], layers[2], layers[-1])\n",
        "    sae3 = _get_sae(layers[2], layers[3], layers[-1])\n",
        "\n",
        "    saes = Sequential()\n",
        "    saes.add(Dense(layers[1], input_dim=layers[0], name='hidden1'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dense(layers[2], name='hidden2'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dense(layers[3], name='hidden3'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dropout(0.2))\n",
        "    saes.add(Dense(layers[4], activation='sigmoid'))\n",
        "\n",
        "    models = [sae1, sae2, sae3, saes]\n",
        "\n",
        "    return models\n",
        "\n",
        "\"\"\"\n",
        "Processing the data\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "def process_data(train, test, lags):\n",
        "    \"\"\"Process data\n",
        "    Reshape and split train\\test data.\n",
        "\n",
        "    # Arguments\n",
        "        train: String, name of .csv train file.\n",
        "        test: String, name of .csv test file.\n",
        "        lags: integer, time lag.\n",
        "    # Returns\n",
        "        X_train: ndarray.\n",
        "        y_train: ndarray.\n",
        "        X_test: ndarray.\n",
        "        y_test: ndarray.\n",
        "        scaler: StandardScaler.\n",
        "    \"\"\"\n",
        "    attr = 'Lane 1 Flow (Veh/5 Minutes)'\n",
        "    df1 = pd.read_csv(train, encoding='utf-8').fillna(0)\n",
        "    df2 = pd.read_csv(test, encoding='utf-8').fillna(0)\n",
        "\n",
        "    # scaler = StandardScaler().fit(df1[attr].values)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(df1[attr].values.reshape(-1, 1))\n",
        "    flow1 = scaler.transform(df1[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "    flow2 = scaler.transform(df2[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
        "\n",
        "    train, test = [], []\n",
        "    for i in range(lags, len(flow1)):\n",
        "        train.append(flow1[i - lags: i + 1])\n",
        "    for i in range(lags, len(flow2)):\n",
        "        test.append(flow2[i - lags: i + 1])\n",
        "\n",
        "    train = np.array(train)\n",
        "    test = np.array(test)\n",
        "    np.random.shuffle(train)\n",
        "\n",
        "    X_train = train[:, :-1]\n",
        "    y_train = train[:, -1]\n",
        "    X_test = test[:, :-1]\n",
        "    y_test = test[:, -1]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "2ivVu8i_Qasa",
        "outputId": "31b729e9-229d-4539-81c8-3e92ebf768de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.recurrent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5cba80d234a3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.recurrent'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traffic Flow Prediction\n",
        "Traffic Flow Prediction with Neural Networks(SAEs、LSTM、GRU).\n",
        "\n",
        "## Requirement\n",
        "- Python 3.6\n",
        "- Tensorflow-gpu 1.5.0\n",
        "- Keras 2.1.3\n",
        "- scikit-learn 0.19\n",
        "\n",
        "## Train the model\n",
        "\n",
        "**Run command below to train the model:**\n",
        "\n",
        "```\n",
        "python train.py --model model_name\n",
        "```\n",
        "\n",
        "You can choose \"lstm\", \"gru\" or \"saes\" as arguments. The ```.h5``` weight file was saved at model folder.\n",
        "\n",
        "\n",
        "## Experiment\n",
        "\n",
        "Data are obtained from the Caltrans Performance Measurement System (PeMS). Data are collected in real-time from individual adetectors spanning the freeway system across all major metropolitan areas of the State of California.\n",
        "\n",
        "\tdevice: Tesla K80\n",
        "\tdataset: PeMS 5min-interval traffic flow data\n",
        "\toptimizer: RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
        "\tbatch_szie: 256\n",
        "\n",
        "\n",
        "**Run command below to run the program:**\n",
        "\n",
        "```\n",
        "python main.py\n",
        "```\n",
        "\n",
        "These are the details for the traffic flow prediction experiment.\n",
        "\n",
        "\n",
        "| Metrics | MAE | MSE | RMSE | MAPE |  R2  | Explained variance score |\n",
        "| ------- |:---:| :--:| :--: | :--: | :--: | :----------------------: |\n",
        "| LSTM | 7.21 | 98.05 | 9.90 | 16.56% | 0.9396 | 0.9419 |\n",
        "| GRU | 7.20 | 99.32 | 9.97| 16.78% | 0.9389 | 0.9389|\n",
        "| SAEs | 7.06 | 92.08 | 9.60 | 17.80% | 0.9433 | 0.9442 |\n",
        "\n",
        "![evaluate](/images/eva.png)\n",
        "\n",
        "## Reference\n",
        "\n",
        "\t@article{SAEs,\n",
        "\t  title={Traffic Flow Prediction With Big Data: A Deep Learning Approach},\n",
        "\t  author={Y Lv, Y Duan, W Kang, Z Li, FY Wang},\n",
        "\t  journal={IEEE Transactions on Intelligent Transportation Systems, 2015, 16(2):865-873},\n",
        "\t  year={2015}\n",
        "\t}\n",
        "\n",
        "\t@article{RNN,\n",
        "\t  title={Using LSTM and GRU neural network methods for traffic flow prediction},\n",
        "\t  author={R Fu, Z Zhang, L Li},\n",
        "\t  journal={Chinese Association of Automation, 2017:324-328},\n",
        "\t  year={2017}\n",
        "\t}\n",
        "\n",
        "\n",
        "## Copyright\n",
        "See [LICENSE](LICENSE) for details.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "9xqw1NgTQk20",
        "outputId": "e0a91106-c158-4f95-9508-1f4c7599d6c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '、' (U+3001) (<ipython-input-11-b4b23fe096c8>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-b4b23fe096c8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Traffic Flow Prediction with Neural Networks(SAEs、LSTM、GRU).\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '、' (U+3001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Defination of NN model\n",
        "\"\"\"\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "def get_lstm(units):\n",
        "    \"\"\"LSTM(Long Short-Term Memory)\n",
        "    Build LSTM Model.\n",
        "\n",
        "    # Arguments\n",
        "        units: List(int), number of input, output and hidden units.\n",
        "    # Returns\n",
        "        model: Model, nn model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
        "    model.add(LSTM(units[2]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units[3], activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_gru(units):\n",
        "    \"\"\"GRU(Gated Recurrent Unit)\n",
        "    Build GRU Model.\n",
        "\n",
        "    # Arguments\n",
        "        units: List(int), number of input, output and hidden units.\n",
        "    # Returns\n",
        "        model: Model, nn model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units[1], input_shape=(units[0], 1), return_sequences=True))\n",
        "    model.add(GRU(units[2]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units[3], activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_sae(inputs, hidden, output):\n",
        "    \"\"\"SAE(Auto-Encoders)\n",
        "    Build SAE Model.\n",
        "\n",
        "    # Arguments\n",
        "        inputs: Integer, number of input units.\n",
        "        hidden: Integer, number of hidden units.\n",
        "        output: Integer, number of output units.\n",
        "    # Returns\n",
        "        model: Model, nn model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden, input_dim=inputs, name='hidden'))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_saes(layers):\n",
        "    \"\"\"SAEs(Stacked Auto-Encoders)\n",
        "    Build SAEs Model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: List(int), number of input, output and hidden units.\n",
        "    # Returns\n",
        "        models: List(Model), List of SAE and SAEs.\n",
        "    \"\"\"\n",
        "    sae1 = _get_sae(layers[0], layers[1], layers[-1])\n",
        "    sae2 = _get_sae(layers[1], layers[2], layers[-1])\n",
        "    sae3 = _get_sae(layers[2], layers[3], layers[-1])\n",
        "\n",
        "    saes = Sequential()\n",
        "    saes.add(Dense(layers[1], input_dim=layers[0], name='hidden1'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dense(layers[2], name='hidden2'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dense(layers[3], name='hidden3'))\n",
        "    saes.add(Activation('sigmoid'))\n",
        "    saes.add(Dropout(0.2))\n",
        "    saes.add(Dense(layers[4], activation='sigmoid'))\n",
        "\n",
        "    models = [sae1, sae2, sae3, saes]\n",
        "\n",
        "    return models\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "GtTPW29GQ4wZ",
        "outputId": "eba34ff4-ff3a-4ee1-a482-14bccfcb1315"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.recurrent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-87fa2ab2cd54>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.recurrent'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}